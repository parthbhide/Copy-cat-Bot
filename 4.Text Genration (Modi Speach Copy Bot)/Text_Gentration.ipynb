{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('english_speeches_date_place_title_text.txt',encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable = ['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1723200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punch(doc_text):\n",
    "    return[ token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n •·']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = seperate_punch(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making lists of 11 words\n",
    "#We are going to set 10 as length of seed text and then going to predict 11th word\n",
    "train_len = 10 + 1\n",
    "text_seq = []\n",
    "#Creates list of lists of 11 words each\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_seq.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences  = tokenizer.texts_to_sequences(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting our vocaboulary size to total number of unique words we have\n",
    "vocab_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to numpy array\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is our feature label\n",
    "#Taking first ten words form the array as our features\n",
    "X = sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking last word of the array as our prediction \n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot-Encoding\n",
    "#num_classes = vocab_size+1 , one extra for padding\n",
    "y = to_categorical(y,num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of words in seed text\n",
    "seq_ln = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeldefination\n",
    "#Takes parameter vocab_size and length of seed text\n",
    "#Using one embedding layer, two LSTM layers and two dense layers\n",
    "#Printing summary and returning model\n",
    "def create_model(vocab_size,seq_ln):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,seq_ln,input_length=seq_ln))\n",
    "    model.add(LSTM(80,return_sequences=True))\n",
    "    model.add(LSTM(80))\n",
    "    model.add(Dense(80,activation='relu'))\n",
    "    model.add(Dense(vocab_size,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 10, 10)            134670    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 10, 80)            29120     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 80)                51520     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 13467)             1090827   \n",
      "=================================================================\n",
      "Total params: 1,312,617\n",
      "Trainable params: 1,312,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size+1,seq_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      " - 333s - loss: 4.6320 - acc: 0.1887\n",
      "Epoch 2/55\n",
      " - 348s - loss: 4.5802 - acc: 0.1909\n",
      "Epoch 3/55\n",
      " - 353s - loss: 4.5332 - acc: 0.1935\n",
      "Epoch 4/55\n",
      " - 355s - loss: 4.4889 - acc: 0.1960\n",
      "Epoch 5/55\n",
      " - 339s - loss: 4.4488 - acc: 0.1981\n",
      "Epoch 6/55\n",
      " - 346s - loss: 4.4106 - acc: 0.2006\n",
      "Epoch 7/55\n",
      " - 355s - loss: 4.3765 - acc: 0.2028\n",
      "Epoch 8/55\n",
      " - 356s - loss: 4.3434 - acc: 0.2056\n",
      "Epoch 9/55\n",
      " - 355s - loss: 4.3147 - acc: 0.2074\n",
      "Epoch 10/55\n",
      " - 352s - loss: 4.2857 - acc: 0.2089\n",
      "Epoch 11/55\n",
      " - 347s - loss: 4.2587 - acc: 0.2115\n",
      "Epoch 12/55\n",
      " - 347s - loss: 4.2312 - acc: 0.2135\n",
      "Epoch 13/55\n",
      " - 354s - loss: 4.2069 - acc: 0.2155\n",
      "Epoch 14/55\n",
      " - 356s - loss: 4.1836 - acc: 0.2169\n",
      "Epoch 15/55\n",
      " - 359s - loss: 4.1617 - acc: 0.2192\n",
      "Epoch 16/55\n",
      " - 363s - loss: 4.1397 - acc: 0.2211\n",
      "Epoch 17/55\n",
      " - 360s - loss: 4.1189 - acc: 0.2226\n",
      "Epoch 18/55\n",
      " - 344s - loss: 4.0999 - acc: 0.2241\n",
      "Epoch 19/55\n",
      " - 345s - loss: 4.0801 - acc: 0.2259\n",
      "Epoch 20/55\n",
      " - 333s - loss: 4.0629 - acc: 0.2277\n",
      "Epoch 21/55\n",
      " - 328s - loss: 4.0465 - acc: 0.2293\n",
      "Epoch 22/55\n",
      " - 331s - loss: 4.0296 - acc: 0.2307\n",
      "Epoch 23/55\n",
      " - 334s - loss: 4.0131 - acc: 0.2315\n",
      "Epoch 24/55\n",
      " - 327s - loss: 3.9966 - acc: 0.2335\n",
      "Epoch 25/55\n",
      " - 352s - loss: 3.9829 - acc: 0.2343\n",
      "Epoch 26/55\n",
      " - 336s - loss: 3.9686 - acc: 0.2364\n",
      "Epoch 27/55\n",
      " - 349s - loss: 3.9550 - acc: 0.2366\n",
      "Epoch 28/55\n",
      " - 485s - loss: 3.9411 - acc: 0.2388\n",
      "Epoch 29/55\n",
      " - 484s - loss: 3.9270 - acc: 0.2398\n",
      "Epoch 30/55\n",
      " - 484s - loss: 3.9153 - acc: 0.2407\n",
      "Epoch 31/55\n",
      " - 502s - loss: 3.9035 - acc: 0.2419\n",
      "Epoch 32/55\n",
      " - 495s - loss: 3.8921 - acc: 0.2429\n",
      "Epoch 33/55\n",
      " - 489s - loss: 3.8812 - acc: 0.2437\n",
      "Epoch 34/55\n",
      " - 483s - loss: 3.8701 - acc: 0.2453\n",
      "Epoch 35/55\n",
      " - 495s - loss: 3.8587 - acc: 0.2465\n",
      "Epoch 36/55\n",
      " - 498s - loss: 3.8480 - acc: 0.2473\n",
      "Epoch 37/55\n",
      " - 463s - loss: 3.8380 - acc: 0.2481\n",
      "Epoch 38/55\n",
      " - 490s - loss: 3.8288 - acc: 0.2493\n",
      "Epoch 39/55\n",
      " - 496s - loss: 3.8162 - acc: 0.2504\n",
      "Epoch 40/55\n",
      " - 361s - loss: 3.8079 - acc: 0.2513\n",
      "Epoch 41/55\n",
      " - 335s - loss: 3.7991 - acc: 0.2521\n",
      "Epoch 42/55\n",
      " - 347s - loss: 3.7889 - acc: 0.2531\n",
      "Epoch 43/55\n",
      " - 355s - loss: 3.7798 - acc: 0.2548\n",
      "Epoch 44/55\n",
      " - 354s - loss: 3.7701 - acc: 0.2556\n",
      "Epoch 45/55\n",
      " - 356s - loss: 3.7622 - acc: 0.2559\n",
      "Epoch 46/55\n",
      " - 361s - loss: 3.7515 - acc: 0.2567\n",
      "Epoch 47/55\n",
      " - 353s - loss: 3.7434 - acc: 0.2583\n",
      "Epoch 48/55\n",
      " - 334s - loss: 3.7344 - acc: 0.2592\n",
      "Epoch 49/55\n",
      " - 345s - loss: 3.7259 - acc: 0.2606\n",
      "Epoch 50/55\n",
      " - 353s - loss: 3.7185 - acc: 0.2609\n",
      "Epoch 51/55\n",
      " - 396s - loss: 3.7081 - acc: 0.2621\n",
      "Epoch 52/55\n",
      " - 365s - loss: 3.7000 - acc: 0.2624\n",
      "Epoch 53/55\n",
      " - 388s - loss: 3.6940 - acc: 0.2642\n",
      "Epoch 54/55\n",
      " - 535s - loss: 3.6871 - acc: 0.2642\n",
      "Epoch 55/55\n",
      " - 525s - loss: 3.6775 - acc: 0.2647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5c308e128>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting model on our data\n",
    "#Batch_size = 64 is just an arbitory number\n",
    "#Training upto 55 epochs\n",
    "#verbose = 2 (has values 0,1,2 for which type of summary you want while training your model)\n",
    "model.fit(X,y,batch_size=64,epochs=55,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Using seed text to generate new text sequence\n",
    "#Genrating words equal to \"num_of_gen_words\"\n",
    "#using pad_seqences for cases where our seed text does not match our seed length of 10 words \n",
    "#pad_sequences truncates the word according to \"pre\" or \"post\" passed to it\n",
    "#Getting the word form tokenizer using the predicied word index\n",
    "#joining whole output and returning it\n",
    "\n",
    "def generated_text(model,tokenizer,seq_ln,seed_text,num_of_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_of_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_ln,truncating='pre')\n",
    "        predicted_wrd_index = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        predicted_wrd = tokenizer.index_word[predicted_wrd_index]\n",
    "        input_text += ' '+predicted_wrd\n",
    "        output_text.append(predicted_wrd)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can give a random seed text from training file itself like --> ' '.join(text_seq[70])\n",
    "#Or give custom seed text like -->\n",
    "seed_text = \"Let me begin by wishing ‘Namaskar’ to all of you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = generated_text(model,tokenizer,seq_ln,seed_text,num_of_gen_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text : --- > Let me begin by wishing ‘Namaskar’ to all of you\n",
      "\n",
      "\n",
      "<<--------------------------------------->>\n",
      "\n",
      "\n",
      "Genrerated text : --- > and thank you for this honour and you will have observed the starting effective role in the world of nuclear and agri vessels we have to do that in the world we have decided to do so as well as a great persona of the world we have a chance to be a very productive forum i am happy to be here to do so as well as a great proof of the world ’s largest and spiritual ranking in the world bank imf is a part of the world we have a new era of the world and the\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seed text : --- > {seed_text}\")\n",
    "print(\"\\n\\n<<--------------------------------------->>\\n\\n\")\n",
    "print(f\"Genrerated text : --- > {gen_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
